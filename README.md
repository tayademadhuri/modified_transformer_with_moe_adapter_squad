# modified_transformer_with_moe_adapter_squad
This is a memory-saving training approach that combines sparsity with a Mixture of Experts (MoE) strategy. This approach significantly reduces GPU memory requirements and lightens the computational load on the CPU. Our experiments confirm that this method delivers the best performance in settings where resources are limited
